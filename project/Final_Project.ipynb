{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1r0oVcA7m-t"
      },
      "source": [
        "INSTALLING TRANSFORMER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-wJsZIhrmUi"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHWw4ZVS8U9L"
      },
      "source": [
        "INSTALLING SPACY AND SPACY TRANSFORMERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGolI6b6rsV6",
        "outputId": "d39f62f4-9f3d-4925-95d3-8b74685e07cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-trf==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.2.0/en_core_web_trf-3.2.0-py3-none-any.whl (460.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 460.2 MB 30 kB/s \n",
            "\u001b[?25hCollecting spacy<3.3.0,>=3.2.0\n",
            "  Downloading spacy-3.2.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 7.3 MB/s \n",
            "\u001b[?25hCollecting spacy-transformers<1.2.0,>=1.1.2\n",
            "  Downloading spacy_transformers-1.1.5-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 167 kB/s \n",
            "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 57.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (21.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.0.6)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
            "\u001b[K     |████████████████████████████████| 457 kB 51.2 MB/s \n",
            "\u001b[?25hCollecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (653 kB)\n",
            "\u001b[K     |████████████████████████████████| 653 kB 47.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (0.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (1.21.6)\n",
            "Collecting typing-extensions<4.0.0.0,>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 57.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (1.0.7)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.0.8)\n",
            "Collecting smart-open<6.0.0,>=5.0.0\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (1.11.0+cu113)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2\n",
            "  Downloading spacy_alignments-0.8.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 51.1 MB/s \n",
            "\u001b[?25hCollecting transformers<4.18.0,>=3.4.0\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 53.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<4.18.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.18.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.18.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4.18.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (0.0.53)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers<4.18.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (0.5.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.18.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.18.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-trf==3.2.0) (2.0.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.18.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.18.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->en-core-web-trf==3.2.0) (1.15.0)\n",
            "Installing collected packages: typing-extensions, catalogue, typer, srsly, smart-open, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, transformers, spacy-alignments, spacy, spacy-transformers, en-core-web-trf\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 6.0.0\n",
            "    Uninstalling smart-open-6.0.0:\n",
            "      Successfully uninstalled smart-open-6.0.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.18.0\n",
            "    Uninstalling transformers-4.18.0:\n",
            "      Successfully uninstalled transformers-4.18.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n",
            "Successfully installed catalogue-2.0.7 en-core-web-trf-3.2.0 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 smart-open-5.2.1 spacy-3.2.4 spacy-alignments-0.8.5 spacy-legacy-3.0.9 spacy-loggers-1.0.2 spacy-transformers-1.1.5 srsly-2.4.3 thinc-8.0.15 transformers-4.17.0 typer-0.4.1 typing-extensions-3.10.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.2.0/en_core_web_trf-3.2.0-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5_PpIGv8Z1r"
      },
      "source": [
        "INSTALLING EN_CORE_WEB_SM FOR ENGLISH MODEL PIPLINE OPTIMIZE FOR CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41r8vwXirunt",
        "outputId": "376e877f-2299-4dea-93d8-44f5b935499b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.64.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n",
            "Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.2.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGBY0vuw8gVD"
      },
      "source": [
        "IMPORT SPACY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dV1Ugv6rvDC"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from transformers import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9TBNlmrvJj"
      },
      "outputs": [],
      "source": [
        "# sample text from Wikipedia\n",
        "text = \"\"\"Rabindranath Tagore FRAS (Bengali: রবীন্দ্রনাথ ঠাকুর, /rəˈbɪndrənɑːt tæˈɡɔːr/ (listen); 7 May 1861 – 7 August 1941) was a Bengali polymath who worked as a poet, writer, playwright, composer, philosopher, social reformer and painter. He reshaped Bengali literature and music as well as Indian art with Contextual Modernism in the late 19th and early 20th centuries. Author of the \"profoundly sensitive, fresh and beautiful\" poetry of Gitanjali, he became in 1913 the first non-European and the first lyricist to win the Nobel Prize in Literature. Tagore's poetic songs were viewed as spiritual and mercurial; however, his \"elegant prose and magical poetry\" remain largely unknown outside Bengal. He was a fellow of the Royal Asiatic Society. Referred to as \"the Bard of Bengal\", Tagore was known by sobriquets: Gurudev, Kobiguru, Biswakobi.[a]\n",
        "\n",
        "A Bengali Brahmin from Calcutta with ancestral gentry roots in Burdwan district[9] and Jessore, Tagore wrote poetry as an eight-year-old. At the age of sixteen, he released his first substantial poems under the pseudonym Bhānusiṃha (\"Sun Lion\"), which were seized upon by literary authorities as long-lost classics. By 1877 he graduated to his first short stories and dramas, published under his real name. As a humanist, universalist, internationalist, and ardent anti-nationalist, he denounced the British Raj and advocated independence from Britain. As an exponent of the Bengal Renaissance, he advanced a vast canon that comprised paintings, sketches and doodles, hundreds of texts, and some two thousand songs; his legacy also endures in his founding of Visva-Bharati University.\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkH9O7ox8njz"
      },
      "source": [
        "USING NER PIPELINE FOR CALLING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iryiIiPVrvMq",
        "outputId": "5bd9c7f5-9e3a-4294-8322-6417fd76fbb3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/dslim/bert-base-NER/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a5ff16a1d557b5ad480f50b1d454448475c644d08df9ce8fccabea7745bebd9f.a61836f2236a3ff1a0827544e2d7c512cbb8cd26ed7b32d643526bebb5d7f92e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"dslim/bert-base-NER\",\n",
            "  \"_num_labels\": 9,\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-MISC\",\n",
            "    \"2\": \"I-MISC\",\n",
            "    \"3\": \"B-PER\",\n",
            "    \"4\": \"I-PER\",\n",
            "    \"5\": \"B-ORG\",\n",
            "    \"6\": \"I-ORG\",\n",
            "    \"7\": \"B-LOC\",\n",
            "    \"8\": \"I-LOC\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 7,\n",
            "    \"B-MISC\": 1,\n",
            "    \"B-ORG\": 5,\n",
            "    \"B-PER\": 3,\n",
            "    \"I-LOC\": 8,\n",
            "    \"I-MISC\": 2,\n",
            "    \"I-ORG\": 6,\n",
            "    \"I-PER\": 4,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/dslim/bert-base-NER/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a5ff16a1d557b5ad480f50b1d454448475c644d08df9ce8fccabea7745bebd9f.a61836f2236a3ff1a0827544e2d7c512cbb8cd26ed7b32d643526bebb5d7f92e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"dslim/bert-base-NER\",\n",
            "  \"_num_labels\": 9,\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-MISC\",\n",
            "    \"2\": \"I-MISC\",\n",
            "    \"3\": \"B-PER\",\n",
            "    \"4\": \"I-PER\",\n",
            "    \"5\": \"B-ORG\",\n",
            "    \"6\": \"I-ORG\",\n",
            "    \"7\": \"B-LOC\",\n",
            "    \"8\": \"I-LOC\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 7,\n",
            "    \"B-MISC\": 1,\n",
            "    \"B-ORG\": 5,\n",
            "    \"B-PER\": 3,\n",
            "    \"I-LOC\": 8,\n",
            "    \"I-MISC\": 2,\n",
            "    \"I-ORG\": 6,\n",
            "    \"I-PER\": 4,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/dslim/bert-base-NER/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/3ca763a5697d51432247d711b6aae51030a05f5b0c9a59cb83b20255eabb7ff4.aeec53fbb8d04bbdb0c84621a6f18491499bffc49a246808de99e63e7684ad79\n",
            "All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "All the weights of BertForTokenClassification were initialized from the model checkpoint at dslim/bert-base-NER.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
            "loading configuration file https://huggingface.co/dslim/bert-base-NER/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a5ff16a1d557b5ad480f50b1d454448475c644d08df9ce8fccabea7745bebd9f.a61836f2236a3ff1a0827544e2d7c512cbb8cd26ed7b32d643526bebb5d7f92e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"dslim/bert-base-NER\",\n",
            "  \"_num_labels\": 9,\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-MISC\",\n",
            "    \"2\": \"I-MISC\",\n",
            "    \"3\": \"B-PER\",\n",
            "    \"4\": \"I-PER\",\n",
            "    \"5\": \"B-ORG\",\n",
            "    \"6\": \"I-ORG\",\n",
            "    \"7\": \"B-LOC\",\n",
            "    \"8\": \"I-LOC\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 7,\n",
            "    \"B-MISC\": 1,\n",
            "    \"B-ORG\": 5,\n",
            "    \"B-PER\": 3,\n",
            "    \"I-LOC\": 8,\n",
            "    \"I-MISC\": 2,\n",
            "    \"I-ORG\": 6,\n",
            "    \"I-PER\": 4,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/dslim/bert-base-NER/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/d426f14ce999ecd9a2f26bd379117e988775a97ca1d30e72941824935563e2a6.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
            "loading file https://huggingface.co/dslim/bert-base-NER/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/dslim/bert-base-NER/resolve/main/added_tokens.json from cache at /root/.cache/huggingface/transformers/256d34bb8f151641e2ce0fcb0263b6652c9ddd412b271fddb03da7d3c6d74448.5cc6e825eb228a7a5cfd27cb4d7151e97a79fb962b31aaf1813aa102e746584b\n",
            "loading file https://huggingface.co/dslim/bert-base-NER/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/8ecdadef2bc275e74e0d4541ae8a5db151fba13174b86dfa88ef5765d30feb77.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "loading file https://huggingface.co/dslim/bert-base-NER/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/de9f40a9d698f5f7227cbc2798430cb498bb680bcd657f1c2bd897a6a2f63953.6391beef2ceed2cdba47401eb12680200856c97d2f2b56143e515d7c0f36a66a\n",
            "loading configuration file https://huggingface.co/dslim/bert-base-NER/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a5ff16a1d557b5ad480f50b1d454448475c644d08df9ce8fccabea7745bebd9f.a61836f2236a3ff1a0827544e2d7c512cbb8cd26ed7b32d643526bebb5d7f92e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"dslim/bert-base-NER\",\n",
            "  \"_num_labels\": 9,\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-MISC\",\n",
            "    \"2\": \"I-MISC\",\n",
            "    \"3\": \"B-PER\",\n",
            "    \"4\": \"I-PER\",\n",
            "    \"5\": \"B-ORG\",\n",
            "    \"6\": \"I-ORG\",\n",
            "    \"7\": \"B-LOC\",\n",
            "    \"8\": \"I-LOC\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 7,\n",
            "    \"B-MISC\": 1,\n",
            "    \"B-ORG\": 5,\n",
            "    \"B-PER\": 3,\n",
            "    \"I-LOC\": 8,\n",
            "    \"I-MISC\": 2,\n",
            "    \"I-ORG\": 6,\n",
            "    \"I-PER\": 4,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/dslim/bert-base-NER/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a5ff16a1d557b5ad480f50b1d454448475c644d08df9ce8fccabea7745bebd9f.a61836f2236a3ff1a0827544e2d7c512cbb8cd26ed7b32d643526bebb5d7f92e\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"dslim/bert-base-NER\",\n",
            "  \"_num_labels\": 9,\n",
            "  \"architectures\": [\n",
            "    \"BertForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"B-MISC\",\n",
            "    \"2\": \"I-MISC\",\n",
            "    \"3\": \"B-PER\",\n",
            "    \"4\": \"I-PER\",\n",
            "    \"5\": \"B-ORG\",\n",
            "    \"6\": \"I-ORG\",\n",
            "    \"7\": \"B-LOC\",\n",
            "    \"8\": \"I-LOC\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 7,\n",
            "    \"B-MISC\": 1,\n",
            "    \"B-ORG\": 5,\n",
            "    \"B-PER\": 3,\n",
            "    \"I-LOC\": 8,\n",
            "    \"I-MISC\": 2,\n",
            "    \"I-ORG\": 6,\n",
            "    \"I-PER\": 4,\n",
            "    \"O\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# load BERT model fine-tuned for Named Entity Recognition (NER)\n",
        "ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWCfaQEi8sbe"
      },
      "source": [
        "EXTRACTING ENTITIES FROM TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky4W48Z7r5s0",
        "outputId": "605cbd1c-071a-466b-dd85-9ead7ff80a19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'end': 2,\n",
              "  'entity': 'B-PER',\n",
              "  'index': 1,\n",
              "  'score': 0.99941504,\n",
              "  'start': 0,\n",
              "  'word': 'Ra'},\n",
              " {'end': 5,\n",
              "  'entity': 'B-PER',\n",
              "  'index': 2,\n",
              "  'score': 0.9878808,\n",
              "  'start': 2,\n",
              "  'word': '##bin'},\n",
              " {'end': 8,\n",
              "  'entity': 'I-PER',\n",
              "  'index': 3,\n",
              "  'score': 0.8254768,\n",
              "  'start': 5,\n",
              "  'word': '##dra'},\n",
              " {'end': 12,\n",
              "  'entity': 'I-PER',\n",
              "  'index': 4,\n",
              "  'score': 0.9565929,\n",
              "  'start': 8,\n",
              "  'word': '##nath'},\n",
              " {'end': 16,\n",
              "  'entity': 'I-PER',\n",
              "  'index': 5,\n",
              "  'score': 0.99897206,\n",
              "  'start': 13,\n",
              "  'word': 'Tag'},\n",
              " {'end': 19,\n",
              "  'entity': 'I-PER',\n",
              "  'index': 6,\n",
              "  'score': 0.9970107,\n",
              "  'start': 16,\n",
              "  'word': '##ore'},\n",
              " {'end': 33,\n",
              "  'entity': 'B-MISC',\n",
              "  'index': 11,\n",
              "  'score': 0.99661094,\n",
              "  'start': 26,\n",
              "  'word': 'Bengali'},\n",
              " {'end': 36,\n",
              "  'entity': 'B-PER',\n",
              "  'index': 13,\n",
              "  'score': 0.9385123,\n",
              "  'start': 35,\n",
              "  'word': 'র'},\n",
              " {'end': 41,\n",
              "  'entity': 'I-PER',\n",
              "  'index': 18,\n",
              "  'score': 0.49223137,\n",
              "  'start': 40,\n",
              "  'word': '##দ'},\n",
              " {'end': 129,\n",
              "  'entity': 'B-MISC',\n",
              "  'index': 61,\n",
              "  'score': 0.9996644,\n",
              "  'start': 122,\n",
              "  'word': 'Bengali'},\n",
              " {'end': 252,\n",
              "  'entity': 'B-MISC',\n",
              "  'index': 90,\n",
              "  'score': 0.9997105,\n",
              "  'start': 245,\n",
              "  'word': 'Bengali'},\n",
              " {'end': 291,\n",
              "  'entity': 'B-MISC',\n",
              "  'index': 97,\n",
              "  'score': 0.99972445,\n",
              "  'start': 285,\n",
              "  'word': 'Indian'},\n",
              " {'end': 304,\n",
              "  'entity': 'B-MISC',\n",
              "  'index': 100,\n",
              "  'score': 0.99058366,\n",
              "  'start': 301,\n",
              "  'word': 'Con'},\n",
              " {'end': 311,\n",
              "  'entity': 'I-MISC',\n",
              "  'index': 102,\n",
              "  'score': 0.9768742,\n",
              "  'start': 308,\n",
              "  'word': '##ual'},\n",
              " {'end': 318,\n",
              "  'entity': 'I-MISC',\n",
              "  'index': 103,\n",
              "  'score': 0.99561465,\n",
              "  'start': 312,\n",
              "  'word': 'Modern'},\n",
              " {'end': 321,\n",
              "  'entity': 'I-MISC',\n",
              "  'index': 104,\n",
              "  'score': 0.99099547,\n",
              "  'start': 318,\n",
              "  'word': '##ism'},\n",
              " {'end': 434,\n",
              "  'entity': 'B-PER',\n",
              "  'index': 128,\n",
              "  'score': 0.99917406,\n",
              "  'start': 433,\n",
              "  'word': 'G'},\n",
              " {'end': 437,\n",
              "  'entity': 'B-PER',\n",
              "  'index': 129,\n",
              "  'score': 0.89461595,\n",
              "  'start': 434,\n",
              "  'word': '##ita'},\n",
              " {'end': 440,\n",
              "  'entity': 'I-PER',\n",
              "  'index': 130,\n",
              "  'score': 0.96610314,\n",
              "  'start': 437,\n",
              "  'word': '##nja'},\n",
              " {'end': 484,\n",
              "  'entity': 'B-MISC',\n",
              "  'index': 141,\n",
              "  'score': 0.99785596,\n",
              "  'start': 476,\n",
              "  'word': 'European'},\n",
              " {'end': 524,\n",
              "  'entity': 'B-MISC',\n",
              "  'index': 149,\n",
              "  'score': 0.9983694,\n",
              "  'start': 519,\n",
              "  'word': 'Nobel'},\n",
              " {'end': 530,\n",
              "  'entity': 'I-MISC',\n",
              "  'index': 150,\n",
              "  'score': 0.99810153,\n",
              "  'start': 525,\n",
              "  'word': 'Prize'},\n",
              " {'end': 533,\n",
              "  'entity': 'I-MISC',\n",
              "  'index': 151,\n",
              "  'score': 0.99765176,\n",
              "  'start': 531,\n",
              "  'word': 'in'},\n",
              " {'end': 544,\n",
              "  'entity': 'I-MISC',\n",
              "  'index': 152,\n",
              "  'score': 0.997769,\n",
              "  'start': 534,\n",
              "  'word': 'Literature'},\n",
              " {'end': 549,\n",
              "  'entity': 'B-PER',\n",
              "  'index': 154,\n",
              "  'score': 0.9993855,\n",
              "  'start': 546,\n",
              "  'word': 'Tag'},\n",
              " {'end': 552,\n",
              "  'entity': 'I-PER',\n",
              "  'index': 155,\n",
              "  'score': 0.975896,\n",
              "  'start': 549,\n",
              "  'word': '##ore'},\n",
              " {'end': 693,\n",
              "  'entity': 'B-LOC',\n",
              "  'index': 184,\n",
              "  'score': 0.9997813,\n",
              "  'start': 687,\n",
              "  'word': 'Bengal'},\n",
              " {'end': 723,\n",
              "  'entity': 'B-ORG',\n",
              "  'index': 192,\n",
              "  'score': 0.9977307,\n",
              "  'start': 718,\n",
              "  'word': 'Royal'},\n",
              " {'end': 731,\n",
              "  'entity': 'I-ORG',\n",
              "  'index': 193,\n",
              "  'score': 0.99918765,\n",
              "  'start': 724,\n",
              "  'word': 'Asiatic'},\n",
              " {'end': 739,\n",
              "  'entity': 'I-ORG',\n",
              "  'index': 194,\n",
              "  'score': 0.9992011,\n",
              "  'start': 732,\n",
              "  'word': 'Society'},\n",
              " {'end': 768,\n",
              "  'entity': 'I-ORG',\n",
              "  'index': 204,\n",
              "  'score': 0.5085946,\n",
              "  'start': 766,\n",
              "  'word': 'of'},\n",
              " {'end': 775,\n",
              "  'entity': 'I-ORG',\n",
              "  'index': 205,\n",
              "  'score': 0.60761684,\n",
              "  'start': 769,\n",
              "  'word': 'Bengal'},\n",
              " {'end': 781,\n",
              "  'entity': 'B-PER',\n",
              "  'index': 208,\n",
              "  'score': 0.99887186,\n",
              "  'start': 778,\n",
              "  'word': 'Tag'},\n",
              " {'end': 784,\n",
              "  'entity': 'I-PER',\n",
              "  'index': 209,\n",
              "  'score': 0.9061009,\n",
              "  'start': 781,\n",
              "  'word': '##ore'},\n",
              " {'end': 814,\n",
              "  'entity': 'B-PER',\n",
              "  'index': 218,\n",
              "  'score': 0.9960932,\n",
              "  'start': 810,\n",
              "  'word': 'Guru'},\n",
              " {'end': 816,\n",
              "  'entity': 'I-PER',\n",
              "  'index': 219,\n",
              "  'score': 0.48215613,\n",
              "  'start': 814,\n",
              "  'word': '##de'},\n",
              " {'end': 821,\n",
              "  'entity': 'B-PER',\n",
              "  'index': 222,\n",
              "  'score': 0.95317334,\n",
              "  'start': 819,\n",
              "  'word': 'Ko'},\n",
              " {'end': 823,\n",
              "  'entity': 'I-LOC',\n",
              "  'index': 223,\n",
              "  'score': 0.6109301,\n",
              "  'start': 821,\n",
              "  'word': '##bi'},\n",
              " {'end': 825,\n",
              "  'entity': 'I-PER',\n",
              "  'index': 224,\n",
              "  'score': 0.5399494,\n",
              "  'start': 823,\n",
              "  'word': '##gu'},\n",
              " {'end': 830,\n",
              "  'entity': 'B-LOC',\n",
              "  'index': 227,\n",
              "  'score': 0.7589194,\n",
              "  'start': 829,\n",
              "  'word': 'B'},\n",
              " {'end': 832,\n",
              "  'entity': 'B-LOC',\n",
              "  'index': 228,\n",
              "  'score': 0.6915249,\n",
              "  'start': 830,\n",
              "  'word': '##is'},\n",
              " {'end': 834,\n",
              "  'entity': 'I-LOC',\n",
              "  'index': 229,\n",
              "  'score': 0.20617728,\n",
              "  'start': 832,\n",
              "  'word': '##wa'},\n",
              " {'end': 836,\n",
              "  'entity': 'I-LOC',\n",
              "  'index': 230,\n",
              "  'score': 0.51744604,\n",
              "  'start': 834,\n",
              "  'word': '##ko'},\n",
              " {'end': 853,\n",
              "  'entity': 'B-MISC',\n",
              "  'index': 237,\n",
              "  'score': 0.99938273,\n",
              "  'start': 846,\n",
              "  'word': 'Bengali'},\n",
              " {'end': 855,\n",
              "  'entity': 'I-MISC',\n",
              "  'index': 238,\n",
              "  'score': 0.83288884,\n",
              "  'start': 854,\n",
              "  'word': 'B'},\n",
              " {'end': 875,\n",
              "  'entity': 'B-LOC',\n",
              "  'index': 242,\n",
              "  'score': 0.99958414,\n",
              "  'start': 867,\n",
              "  'word': 'Calcutta'},\n",
              " {'end': 908,\n",
              "  'entity': 'B-LOC',\n",
              "  'index': 249,\n",
              "  'score': 0.99748766,\n",
              "  'start': 907,\n",
              "  'word': 'B'},\n",
              " {'end': 910,\n",
              "  'entity': 'B-LOC',\n",
              "  'index': 250,\n",
              "  'score': 0.8390564,\n",
              "  'start': 908,\n",
              "  'word': '##ur'},\n",
              " {'end': 911,\n",
              "  'entity': 'I-LOC',\n",
              "  'index': 251,\n",
              "  'score': 0.78853774,\n",
              "  'start': 910,\n",
              "  'word': '##d'},\n",
              " {'end': 914,\n",
              "  'entity': 'I-LOC',\n",
              "  'index': 252,\n",
              "  'score': 0.9830625,\n",
              "  'start': 911,\n",
              "  'word': '##wan'},\n",
              " {'end': 935,\n",
              "  'entity': 'B-LOC',\n",
              "  'index': 258,\n",
              "  'score': 0.80650455,\n",
              "  'start': 931,\n",
              "  'word': 'Jess'},\n",
              " {'end': 938,\n",
              "  'entity': 'I-LOC',\n",
              "  'index': 259,\n",
              "  'score': 0.9953402,\n",
              "  'start': 935,\n",
              "  'word': '##ore'},\n",
              " {'end': 943,\n",
              "  'entity': 'B-PER',\n",
              "  'index': 261,\n",
              "  'score': 0.9990834,\n",
              "  'start': 940,\n",
              "  'word': 'Tag'},\n",
              " {'end': 946,\n",
              "  'entity': 'I-PER',\n",
              "  'index': 262,\n",
              "  'score': 0.8830466,\n",
              "  'start': 943,\n",
              "  'word': '##ore'},\n",
              " {'end': 1066,\n",
              "  'entity': 'B-LOC',\n",
              "  'index': 288,\n",
              "  'score': 0.51082647,\n",
              "  'start': 1065,\n",
              "  'word': 'B'},\n",
              " {'end': 1081,\n",
              "  'entity': 'B-MISC',\n",
              "  'index': 297,\n",
              "  'score': 0.70375556,\n",
              "  'start': 1078,\n",
              "  'word': 'Sun'},\n",
              " {'end': 1086,\n",
              "  'entity': 'I-MISC',\n",
              "  'index': 298,\n",
              "  'score': 0.7927515,\n",
              "  'start': 1082,\n",
              "  'word': 'Lion'},\n",
              " {'end': 1351,\n",
              "  'entity': 'B-MISC',\n",
              "  'index': 355,\n",
              "  'score': 0.9991731,\n",
              "  'start': 1344,\n",
              "  'word': 'British'},\n",
              " {'end': 1355,\n",
              "  'entity': 'I-MISC',\n",
              "  'index': 356,\n",
              "  'score': 0.91008824,\n",
              "  'start': 1352,\n",
              "  'word': 'Raj'},\n",
              " {'end': 1395,\n",
              "  'entity': 'B-LOC',\n",
              "  'index': 361,\n",
              "  'score': 0.99968076,\n",
              "  'start': 1388,\n",
              "  'word': 'Britain'},\n",
              " {'end': 1425,\n",
              "  'entity': 'B-MISC',\n",
              "  'index': 370,\n",
              "  'score': 0.9988549,\n",
              "  'start': 1419,\n",
              "  'word': 'Bengal'},\n",
              " {'end': 1437,\n",
              "  'entity': 'I-MISC',\n",
              "  'index': 371,\n",
              "  'score': 0.99387133,\n",
              "  'start': 1426,\n",
              "  'word': 'Renaissance'}]"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# perform inference on the transformer model\n",
        "doc_ner = ner(text)\n",
        "# print the output\n",
        "doc_ner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3tq18zD87Aj"
      },
      "source": [
        "Next, let's make a function that uses spaCy to visualize this Python dictionary:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyFIQTG_r5v5"
      },
      "outputs": [],
      "source": [
        "def get_entities_html(text, ner_result, title=None):\n",
        "  \"\"\"Visualize NER with the help of SpaCy\"\"\"\n",
        "  ents = []\n",
        "  for ent in ner_result:\n",
        "    e = {}\n",
        "    # add the start and end positions of the entity\n",
        "    e[\"start\"] = ent[\"start\"]\n",
        "    e[\"end\"] = ent[\"end\"]\n",
        "    # add the score if you want in the label\n",
        "    # e[\"label\"] = f\"{ent[\"entity\"]}-{ent['score']:.2f}\"\n",
        "    e[\"label\"] = ent[\"entity\"]\n",
        "    if ents and -1 <= ent[\"start\"] - ents[-1][\"end\"] <= 1 and ents[-1][\"label\"] == e[\"label\"]:\n",
        "      # if the current entity is shared with previous entity\n",
        "      # simply extend the entity end position instead of adding a new one\n",
        "      ents[-1][\"end\"] = e[\"end\"]\n",
        "      continue\n",
        "    ents.append(e)\n",
        "  # construct data required for displacy.render() method\n",
        "  render_data = [\n",
        "    {\n",
        "      \"text\": text,\n",
        "      \"ents\": ents,\n",
        "      \"title\": title,\n",
        "    }\n",
        "  ]\n",
        "  spacy.displacy.render(render_data, style=\"ent\", manual=True, jupyter=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "7on9bSlIsCG1",
        "outputId": "c6ab7e62-280f-48fa-d10a-d822be4725ae"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Rabin\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
              "</mark>\n",
              "\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    dranath Tagore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              " FRAS (\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
              "</mark>\n",
              ": \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    র\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
              "</mark>\n",
              "বীন্\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    দ\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              "্রনাথ ঠাকুর, /rəˈbɪndrənɑːt tæˈɡɔːr/ (listen); 7 May 1861 – 7 August 1941) was a \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
              "</mark>\n",
              " polymath who worked as a poet, writer, playwright, composer, philosopher, social reformer and painter. He reshaped \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
              "</mark>\n",
              " literature and music as well as \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Indian\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
              "</mark>\n",
              " art with \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Con\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
              "</mark>\n",
              "text\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ual Modernism\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
              "</mark>\n",
              " in the late 19th and early 20th centuries. Author of the &quot;profoundly sensitive, fresh and beautiful&quot; poetry of \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Gita\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
              "</mark>\n",
              "\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    nja\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              "li, he became in 1913 the first non-\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    European\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
              "</mark>\n",
              " and the first lyricist to win the \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Nobel\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Prize in Literature\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
              "</mark>\n",
              ". \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tag\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
              "</mark>\n",
              "\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              "'s poetic songs were viewed as spiritual and mercurial; however, his &quot;elegant prose and magical poetry&quot; remain largely unknown outside \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengal\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-LOC</span>\n",
              "</mark>\n",
              ". He was a fellow of the \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Royal\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-ORG</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Asiatic Society\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-ORG</span>\n",
              "</mark>\n",
              ". Referred to as &quot;the Bard \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    of Bengal\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-ORG</span>\n",
              "</mark>\n",
              "&quot;, \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tag\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
              "</mark>\n",
              "\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              " was known by sobriquets: \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Guru\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
              "</mark>\n",
              "\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    de\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              "v, \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Ko\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
              "</mark>\n",
              "\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    bi\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-LOC</span>\n",
              "</mark>\n",
              "\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    gu\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              "ru, \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bis\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-LOC</span>\n",
              "</mark>\n",
              "\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    wako\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-LOC</span>\n",
              "</mark>\n",
              "bi.[a]</br></br>A \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    B\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
              "</mark>\n",
              "rahmin from \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Calcutta\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-LOC</span>\n",
              "</mark>\n",
              " with ancestral gentry roots in \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bur\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-LOC</span>\n",
              "</mark>\n",
              "\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    dwan\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-LOC</span>\n",
              "</mark>\n",
              " district[9] and \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Jess\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-LOC</span>\n",
              "</mark>\n",
              "\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-LOC</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tag\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-PER</span>\n",
              "</mark>\n",
              "\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              " wrote poetry as an eight-year-old. At the age of sixteen, he released his first substantial poems under the pseudonym \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    B\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-LOC</span>\n",
              "</mark>\n",
              "hānusiṃha (&quot;\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Sun\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Lion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
              "</mark>\n",
              "&quot;), which were seized upon by literary authorities as long-lost classics. By 1877 he graduated to his first short stories and dramas, published under his real name. As a humanist, universalist, internationalist, and ardent anti-nationalist, he denounced the \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    British\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Raj\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
              "</mark>\n",
              " and advocated independence from \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Britain\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-LOC</span>\n",
              "</mark>\n",
              ". As an exponent of the \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengal\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">B-MISC</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Renaissance\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
              "</mark>\n",
              ", he advanced a vast canon that comprised paintings, sketches and doodles, hundreds of texts, and some two thousand songs; his legacy also endures in his founding of Visva-Bharati University.</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# get HTML representation of NER of our text\n",
        "get_entities_html(text, doc_ner)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqnGsMt17leK"
      },
      "source": [
        "O: Outside of a named entity. B-MIS: Beginning of a miscellaneous entity right after another miscellaneous entity. I-MIS: Miscellaneous entity. B-PER: Beginning of a person’s name right after another person’s name. I-PER: Person’s name. B-ORG: The beginning of an organization right after another organization. I-ORG: Organization. B-LOC: Beginning of a location right after another location. I-LOC: Location.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9qnFh4F8_1q"
      },
      "source": [
        "INSTALLING ROBERTA A BETTER MODEL TO CHECK\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S5ghVMHr50n",
        "outputId": "fb1440d8-2f05-4520-a761-9db1e0717720"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/xlm-roberta-large-finetuned-conll03-english/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/cd7e3332fb70fd94304b0fcad297caff7b8d9c97c5f76b5a4ac6bbcc14379fe1.ed0120fc465ef220e4bd136ae002fa78741a9545246ccb78502333b8dba60ee3\n",
            "Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-large-finetuned-conll03-english\",\n",
            "  \"_num_labels\": 8,\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B-LOC\",\n",
            "    \"1\": \"B-MISC\",\n",
            "    \"2\": \"B-ORG\",\n",
            "    \"3\": \"I-LOC\",\n",
            "    \"4\": \"I-MISC\",\n",
            "    \"5\": \"I-ORG\",\n",
            "    \"6\": \"I-PER\",\n",
            "    \"7\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 0,\n",
            "    \"B-MISC\": 1,\n",
            "    \"B-ORG\": 2,\n",
            "    \"I-LOC\": 3,\n",
            "    \"I-MISC\": 4,\n",
            "    \"I-ORG\": 5,\n",
            "    \"I-PER\": 6,\n",
            "    \"O\": 7\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/xlm-roberta-large-finetuned-conll03-english/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/cd7e3332fb70fd94304b0fcad297caff7b8d9c97c5f76b5a4ac6bbcc14379fe1.ed0120fc465ef220e4bd136ae002fa78741a9545246ccb78502333b8dba60ee3\n",
            "Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-large-finetuned-conll03-english\",\n",
            "  \"_num_labels\": 8,\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B-LOC\",\n",
            "    \"1\": \"B-MISC\",\n",
            "    \"2\": \"B-ORG\",\n",
            "    \"3\": \"I-LOC\",\n",
            "    \"4\": \"I-MISC\",\n",
            "    \"5\": \"I-ORG\",\n",
            "    \"6\": \"I-PER\",\n",
            "    \"7\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 0,\n",
            "    \"B-MISC\": 1,\n",
            "    \"B-ORG\": 2,\n",
            "    \"I-LOC\": 3,\n",
            "    \"I-MISC\": 4,\n",
            "    \"I-ORG\": 5,\n",
            "    \"I-PER\": 6,\n",
            "    \"O\": 7\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/xlm-roberta-large-finetuned-conll03-english/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/2b449384f083d8866edacbf92e4011e3fcf3026d45f2b9a4cb8650fec7a525c7.09ff0f236572ba82656162f6d6e7ec75e1af5babbf6a088165855208ad7a2c6d\n",
            "All model checkpoint weights were used when initializing XLMRobertaForTokenClassification.\n",
            "\n",
            "All the weights of XLMRobertaForTokenClassification were initialized from the model checkpoint at xlm-roberta-large-finetuned-conll03-english.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForTokenClassification for predictions without further training.\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/xlm-roberta-large-finetuned-conll03-english/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/cd7e3332fb70fd94304b0fcad297caff7b8d9c97c5f76b5a4ac6bbcc14379fe1.ed0120fc465ef220e4bd136ae002fa78741a9545246ccb78502333b8dba60ee3\n",
            "Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-large-finetuned-conll03-english\",\n",
            "  \"_num_labels\": 8,\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B-LOC\",\n",
            "    \"1\": \"B-MISC\",\n",
            "    \"2\": \"B-ORG\",\n",
            "    \"3\": \"I-LOC\",\n",
            "    \"4\": \"I-MISC\",\n",
            "    \"5\": \"I-ORG\",\n",
            "    \"6\": \"I-PER\",\n",
            "    \"7\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 0,\n",
            "    \"B-MISC\": 1,\n",
            "    \"B-ORG\": 2,\n",
            "    \"I-LOC\": 3,\n",
            "    \"I-MISC\": 4,\n",
            "    \"I-ORG\": 5,\n",
            "    \"I-PER\": 6,\n",
            "    \"O\": 7\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/xlm-roberta-large-finetuned-conll03-english/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/c5604a1be93150d0d1b8dfa45818ac04c1261ea33b5aa73e9f62b07171cafd93.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
            "loading file https://huggingface.co/xlm-roberta-large-finetuned-conll03-english/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/b32bffe3a0720df8952ddee5d7b1f9dd5c71ac8f0e0a69ba69432d8ab0be410c.1b58f47e7fc4532adbdc01d216b9bb2fb0657db965b423d3c9c974934c7c50e3\n",
            "loading file https://huggingface.co/xlm-roberta-large-finetuned-conll03-english/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/xlm-roberta-large-finetuned-conll03-english/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/xlm-roberta-large-finetuned-conll03-english/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/xlm-roberta-large-finetuned-conll03-english/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/cd7e3332fb70fd94304b0fcad297caff7b8d9c97c5f76b5a4ac6bbcc14379fe1.ed0120fc465ef220e4bd136ae002fa78741a9545246ccb78502333b8dba60ee3\n",
            "Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-large-finetuned-conll03-english\",\n",
            "  \"_num_labels\": 8,\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B-LOC\",\n",
            "    \"1\": \"B-MISC\",\n",
            "    \"2\": \"B-ORG\",\n",
            "    \"3\": \"I-LOC\",\n",
            "    \"4\": \"I-MISC\",\n",
            "    \"5\": \"I-ORG\",\n",
            "    \"6\": \"I-PER\",\n",
            "    \"7\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"B-LOC\": 0,\n",
            "    \"B-MISC\": 1,\n",
            "    \"B-ORG\": 2,\n",
            "    \"I-LOC\": 3,\n",
            "    \"I-MISC\": 4,\n",
            "    \"I-ORG\": 5,\n",
            "    \"I-PER\": 6,\n",
            "    \"O\": 7\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# load roberta-large model\n",
        "ner2 = pipeline(\"ner\", model=\"xlm-roberta-large-finetuned-conll03-english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUKv38X6sFsQ"
      },
      "outputs": [],
      "source": [
        "# perform inference on this model\n",
        "doc_ner2 = ner2(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "gzOYSCkxsFvF",
        "outputId": "76c5705f-31b8-4547-e02d-36a04156a0d8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Rabindranath Tagore FRAS\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              " (\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
              "</mark>\n",
              ": \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    রবীন্দ্রনাথ ঠাকুর\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              ", /\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    rə\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              "ˈbɪndrənɑːt \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    t\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              "æˈɡɔːr/ (listen); 7 May 1861 – 7 August 1941) was a \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
              "</mark>\n",
              " polymath who worked as a poet, writer, playwright, composer, philosopher, social reformer and painter. He reshaped \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
              "</mark>\n",
              " literature and music as well as \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Indian\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
              "</mark>\n",
              " art with \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Contextual Modernism\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
              "</mark>\n",
              " in the late 19th and early 20th centuries. Author of the &quot;profoundly sensitive, fresh and beautiful&quot; poetry of \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Gitanjali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              ", he became in 1913 the first \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    non-European\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
              "</mark>\n",
              " and the first lyricist to win the \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Nobel Prize in Literature\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
              "</mark>\n",
              ". \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tagore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              "'s poetic songs were viewed as spiritual and mercurial; however, his &quot;elegant prose and magical poetry&quot; remain largely unknown outside \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengal\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-LOC</span>\n",
              "</mark>\n",
              ". He was a fellow of the \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Royal Asiatic Society\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-ORG</span>\n",
              "</mark>\n",
              ". Referred to as &quot;the \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bard\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              " of \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengal\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-LOC</span>\n",
              "</mark>\n",
              "&quot;, \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tagore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              " was known by sobriquets: \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Gurudev\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Kobiguru\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Biswakobi\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              ".[a]</br></br>A \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali Brah\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
              "</mark>\n",
              "min from \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Calcutta\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-LOC</span>\n",
              "</mark>\n",
              " with ancestral gentry roots in \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Burdwan\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-LOC</span>\n",
              "</mark>\n",
              " district[9] and \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Jessore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-LOC</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tagore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              " wrote poetry as an eight-year-old. At the age of sixteen, he released his first substantial poems under the pseudonym \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bhānusiṃha\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              " (&quot;\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Sun Lion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-PER</span>\n",
              "</mark>\n",
              "&quot;), which were seized upon by literary authorities as long-lost classics. By 1877 he graduated to his first short stories and dramas, published under his real name. As a humanist, universalist, internationalist, and ardent anti-nationalist, he denounced the \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    British Raj\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
              "</mark>\n",
              " and advocated independence from \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Britain\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-LOC</span>\n",
              "</mark>\n",
              ". As an exponent of the \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengal Renaissance\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-MISC</span>\n",
              "</mark>\n",
              ", he advanced a vast canon that comprised paintings, sketches and doodles, hundreds of texts, and some two thousand songs; his legacy also endures in his founding of \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Visva-Bharati University\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">I-ORG</span>\n",
              "</mark>\n",
              ".</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# get HTML representation of NER of our text\n",
        "get_entities_html(text, doc_ner2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sAp5Ybl9ZJt"
      },
      "source": [
        "As you can see, now it's improved, naming Rabindranath Tagore as a single entity and also the district Jessore.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kb7QoqYVsL0s",
        "outputId": "4b5f699a-3f52-423e-dc33-33f30843b206"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/Jean-Baptiste/roberta-large-ner-english/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8dfd6e6f356a5aa80b89f733ff8f5482e75900ac4119263e4546df520e9f4407.17dd1ad9b326768978c0c8d481df74561f1d4c138260b2771389ffbac2bfc616\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"Jean-Baptiste/roberta-large-ner-english\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"PER\",\n",
            "    \"2\": \"ORG\",\n",
            "    \"3\": \"LOC\",\n",
            "    \"4\": \"MISC\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LOC\": 3,\n",
            "    \"MISC\": 4,\n",
            "    \"O\": 0,\n",
            "    \"ORG\": 2,\n",
            "    \"PER\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/Jean-Baptiste/roberta-large-ner-english/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8dfd6e6f356a5aa80b89f733ff8f5482e75900ac4119263e4546df520e9f4407.17dd1ad9b326768978c0c8d481df74561f1d4c138260b2771389ffbac2bfc616\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"Jean-Baptiste/roberta-large-ner-english\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"PER\",\n",
            "    \"2\": \"ORG\",\n",
            "    \"3\": \"LOC\",\n",
            "    \"4\": \"MISC\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LOC\": 3,\n",
            "    \"MISC\": 4,\n",
            "    \"O\": 0,\n",
            "    \"ORG\": 2,\n",
            "    \"PER\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/Jean-Baptiste/roberta-large-ner-english/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7cd6d28bedc175df9fb66d27c29f232222ace92dca20ed45e05a27a2fdd50f56.84a94b998a1b076bfba1ce0dbbb2d8eb1afe70835aa0ab6d1d1a92aa94cd0b40\n",
            "All model checkpoint weights were used when initializing RobertaForTokenClassification.\n",
            "\n",
            "All the weights of RobertaForTokenClassification were initialized from the model checkpoint at Jean-Baptiste/roberta-large-ner-english.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForTokenClassification for predictions without further training.\n",
            "loading configuration file https://huggingface.co/Jean-Baptiste/roberta-large-ner-english/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8dfd6e6f356a5aa80b89f733ff8f5482e75900ac4119263e4546df520e9f4407.17dd1ad9b326768978c0c8d481df74561f1d4c138260b2771389ffbac2bfc616\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"Jean-Baptiste/roberta-large-ner-english\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"PER\",\n",
            "    \"2\": \"ORG\",\n",
            "    \"3\": \"LOC\",\n",
            "    \"4\": \"MISC\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LOC\": 3,\n",
            "    \"MISC\": 4,\n",
            "    \"O\": 0,\n",
            "    \"ORG\": 2,\n",
            "    \"PER\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/Jean-Baptiste/roberta-large-ner-english/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/ea0da06ea123e2bbf6bc646ace42ce2b5e427e03325288ed69150197077fac56.bfdcc444ff249bca1a95ca170ec350b442f81804d7df3a95a2252217574121d7\n",
            "loading file https://huggingface.co/Jean-Baptiste/roberta-large-ner-english/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/71a89f7aee3ad4e065420256705d7c1fa2dd95d6ef7c18904ee247a6357ec201.f5b91da9e34259b8f4d88dbc97c740667a0e8430b96314460cdb04e86d4fc435\n",
            "loading file https://huggingface.co/Jean-Baptiste/roberta-large-ner-english/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/Jean-Baptiste/roberta-large-ner-english/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/Jean-Baptiste/roberta-large-ner-english/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/386bbab29d21346a601dce02866bc2e2bc3c232da1d075cec3896bcabfabf227.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
            "loading file https://huggingface.co/Jean-Baptiste/roberta-large-ner-english/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/dab927a4521cc28698b04a671a96cc167180d7cb6d0a4c0da9f88385cf4a901e.1198ac0df8aade842fa41e634e59c5f5098947fc60d581929c8f0a2d96c3becd\n",
            "loading configuration file https://huggingface.co/Jean-Baptiste/roberta-large-ner-english/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8dfd6e6f356a5aa80b89f733ff8f5482e75900ac4119263e4546df520e9f4407.17dd1ad9b326768978c0c8d481df74561f1d4c138260b2771389ffbac2bfc616\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"Jean-Baptiste/roberta-large-ner-english\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"PER\",\n",
            "    \"2\": \"ORG\",\n",
            "    \"3\": \"LOC\",\n",
            "    \"4\": \"MISC\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LOC\": 3,\n",
            "    \"MISC\": 4,\n",
            "    \"O\": 0,\n",
            "    \"ORG\": 2,\n",
            "    \"PER\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/Jean-Baptiste/roberta-large-ner-english/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/8dfd6e6f356a5aa80b89f733ff8f5482e75900ac4119263e4546df520e9f4407.17dd1ad9b326768978c0c8d481df74561f1d4c138260b2771389ffbac2bfc616\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"Jean-Baptiste/roberta-large-ner-english\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForTokenClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"O\",\n",
            "    \"1\": \"PER\",\n",
            "    \"2\": \"ORG\",\n",
            "    \"3\": \"LOC\",\n",
            "    \"4\": \"MISC\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LOC\": 3,\n",
            "    \"MISC\": 4,\n",
            "    \"O\": 0,\n",
            "    \"ORG\": 2,\n",
            "    \"PER\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Rabindranath Tagore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    FRAS\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " (\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
              "</mark>\n",
              ": রবীন্দ্রনাথ ঠাকুর, /\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    rəˈbɪndrənɑːt t\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              "æˈɡɔː\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    r\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              "/ (listen); 7 May 1861 – 7 August 1941) was a \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
              "</mark>\n",
              " polymath who worked as a poet, writer, playwright, composer, philosopher, social reformer and painter. He reshaped \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
              "</mark>\n",
              " literature and music as well as \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Indian\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
              "</mark>\n",
              " art with \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Contextual Modernism\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
              "</mark>\n",
              " in the late 19th and early 20th centuries. Author of the &quot;profoundly sensitive, fresh and beautiful&quot; poetry of \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Gitanjali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
              "</mark>\n",
              ", he became in 1913 the first \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    non-European\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
              "</mark>\n",
              " and the first lyricist to win the \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Nobel Prize in Literature\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
              "</mark>\n",
              ". \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tagore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              "'s poetic songs were viewed as spiritual and mercurial; however, his &quot;elegant prose and magical poetry&quot; remain largely unknown outside \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengal\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ". He was a fellow of the \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Royal Asiatic Society\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ". Referred to as &quot;the \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bard of Bengal\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              "&quot;, \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tagore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " was known by sobriquets: \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Gurudev\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Kobiguru\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Biswakobi\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              ".[a]</br></br>A \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali Brahmin\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
              "</mark>\n",
              " from \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Calcutta\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " with ancestral gentry roots in \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Burdwan\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " district[9] and \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Jessore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tagore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " wrote poetry as an eight-year-old. At the age of sixteen, he released his first substantial poems under the pseudonym \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bhānusiṃha\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              " (&quot;\n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Sun Lion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
              "</mark>\n",
              "&quot;), which were seized upon by literary authorities as long-lost classics. By 1877 he graduated to his first short stories and dramas, published under his real name. As a humanist, universalist, internationalist, and ardent anti-nationalist, he denounced the \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    British Raj\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
              "</mark>\n",
              " and advocated independence from \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Britain\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ". As an exponent of the \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengal Renaissance\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
              "</mark>\n",
              ", he advanced a vast canon that comprised paintings, sketches and doodles, hundreds of texts, and some two thousand songs; his legacy also endures in his founding of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Visva-Bharati University\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ".</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#There are a lot of other models that were fine-tuned on the same dataset. Here's yet another one:\n",
        "\n",
        "# load yet another roberta-large model\n",
        "ner3 = pipeline(\"ner\", model=\"Jean-Baptiste/roberta-large-ner-english\")\n",
        "# perform inference on this model\n",
        "doc_ner3 = ner3(text)\n",
        "# get HTML representation of NER of our text\n",
        "get_entities_html(text, doc_ner3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9lS5IxD-HsH"
      },
      "source": [
        "This model, however, only has PER, MISC, LOC, and ORG entities. SpaCy automatically colors the familiar entities.\n",
        "\n",
        "To perform NER using SpaCy, we must first load the model using spacy.load() function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCYEnq_DsL4D"
      },
      "outputs": [],
      "source": [
        "# load the English CPU-optimized pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exc38U1qsSCX"
      },
      "outputs": [],
      "source": [
        "#We're loading the model we've downloaded. Make sure you download the model you want to use before loading it here. Next, let's generate our document:\n",
        "\n",
        "# predict the entities\n",
        "doc = nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "wYZphu7hsSF9",
        "outputId": "432cdba0-ca5b-425f-aebf-c055531f8001"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Rabindranath Tagore FRAS (\n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              ": \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    রবীন্দ্রনাথ ঠাকুর\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", /rəˈbɪndrənɑːt tæˈɡɔːr/ (listen); 7 \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    May 1861\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " – 7 \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    August 1941\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ") was a \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " polymath who worked as a poet, writer, playwright, composer, philosopher, social reformer and painter. He reshaped \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " literature and music as well as \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Indian\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " art with Contextual Modernism in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the late 19th and early 20th centuries\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ". Author of the &quot;profoundly sensitive, fresh and beautiful&quot; poetry of \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Gitanjali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", he became in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    1913\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " the \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    first\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    non-European\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " and the \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    first\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " lyricist to win \n",
              "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the Nobel Prize in Literature\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
              "</mark>\n",
              ". Tagore's poetic songs were viewed as spiritual and mercurial; however, his &quot;elegant prose and magical poetry&quot; remain largely unknown outside \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengal\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ". He was a fellow of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the Royal Asiatic Society\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ". Referred to as &quot;the Bard of Bengal&quot;, \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tagore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " was known by sobriquets: \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Gurudev\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Kobiguru\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", Biswakobi.[a]</br></br>A \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " Brahmin from \n",
              "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Calcutta\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
              "</mark>\n",
              " with ancestral gentry roots in Burdwan district[9] and \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Jessore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tagore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " wrote poetry as an \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    eight-year-old\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ". At \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the age of sixteen\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", he released his \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    first\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " substantial poems under the pseudonym \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bhānusiṃha\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " (&quot;\n",
              "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Sun Lion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
              "</mark>\n",
              "&quot;), which were seized upon by literary authorities as long-lost classics. By \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    1877\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " he graduated to his \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    first\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " short stories and dramas, published under his real name. As a humanist, universalist, internationalist, and ardent anti-nationalist, he denounced the \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    British\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " Raj and advocated independence from \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Britain\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ". As an exponent of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the Bengal Renaissance\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", he advanced a vast canon that comprised paintings, sketches and doodles, \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    hundreds\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " of texts, and \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    some two thousand\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " songs; his legacy also endures in his founding of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Visva-Bharati University\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ".</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# display the doc with jupyter mode\n",
        "spacy.displacy.render(doc, style=\"ent\", jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPhlxMVw-iEY"
      },
      "source": [
        "This one looks much better, and there are a lot more entities (18) than the previous ones, namely CARDINAL, DATE, EVENT, FAC, GPE, LANGUAGE, LAW, LOC, MONEY, NORP, ORDINAL, ORG, PERCENT, PERSON, PRODUCT, QUANTITY, TIME, WORK_OF_ART\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFADo3IisYaK",
        "outputId": "5a7b514a-2607-4282-d762-86b18eebed9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file /tmp/tmpmempe9u1/config.json\n",
            "Model config RobertaConfig {\n",
            "  \"_name_or_path\": \"/tmp/tmpmempe9u1/config.json\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Didn't find file /tmp/tmpmempe9u1/added_tokens.json. We won't load it.\n",
            "loading file /tmp/tmpmempe9u1/vocab.json\n",
            "loading file /tmp/tmpmempe9u1/merges.txt\n",
            "loading file /tmp/tmpmempe9u1/tokenizer.json\n",
            "loading file None\n",
            "loading file /tmp/tmpmempe9u1/special_tokens_map.json\n",
            "loading file /tmp/tmpmempe9u1/tokenizer_config.json\n"
          ]
        }
      ],
      "source": [
        "#However, Calcutta was mistakenly labeled as an product, so let's use the Transformer model that spaCy is offering:\n",
        "\n",
        "# load the English transformer pipeline (roberta-base) using spaCy\n",
        "nlp_trf = spacy.load('en_core_web_trf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "HryD1S--sbb6",
        "outputId": "1a04ba8e-f34a-44c9-cbe1-8fe392547925"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Rabindranath Tagore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " FRAS (\n",
              "<mark class=\"entity\" style=\"background: #ff8197; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LANGUAGE</span>\n",
              "</mark>\n",
              ": রবীন্দ্রনাথ ঠাকুর, /rəˈbɪndrənɑːt tæˈɡɔːr/ (listen); \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    7 May 1861 –\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " 7 August 1941) was a \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " polymath who worked as a poet, writer, playwright, composer, philosopher, social reformer and painter. He reshaped \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " literature and music as well as \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Indian\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " art with Contextual Modernism in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the late 19th and early 20th centuries\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ". Author of the &quot;profoundly sensitive, fresh and beautiful&quot; poetry of \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Gitanjali\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", he became in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    1913\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " the \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    first\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    non-European\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " and the \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    first\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " lyricist to win \n",
              "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the Nobel Prize in Literature\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
              "</mark>\n",
              ". \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tagore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              "'s poetic songs were viewed as spiritual and mercurial; however, his &quot;elegant prose and magical poetry&quot; remain largely unknown outside \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengal\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ". He was a fellow of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the Royal Asiatic Society\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ". Referred to as &quot;\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the Bard of Bengal\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              "&quot;, \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tagore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " was known by sobriquets: \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Gurudev\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Kobiguru\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Biswakobi.[a\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              "]</br></br>A \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengali Brahmin\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " from \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Calcutta\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " with ancestral gentry roots in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Burdwan\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " district[9] and \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Jessore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tagore\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " wrote poetry as \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    an eight-year-old\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ". At \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the age of sixteen\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", he released his \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    first\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " substantial poems under the pseudonym \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bhānusiṃha\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " (&quot;Sun Lion&quot;), which were seized upon by literary authorities as long-lost classics. By \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    1877\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " he graduated to his \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    first\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " short stories and dramas, published under his real name. As a humanist, universalist, internationalist, and ardent anti-nationalist, he denounced \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    the British Raj\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " and advocated independence from \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Britain\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ". As an exponent of the \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Bengal\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " Renaissance, he advanced a vast canon that comprised paintings, sketches and doodles, \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    hundreds\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " of texts, and \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    some two thousand\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " songs; his legacy also endures in his founding of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Visva-Bharati University\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ".</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Let's perform inference and visualize the text:\n",
        "\n",
        "# perform inference on the model\n",
        "doc_trf = nlp_trf(text)\n",
        "# display the doc with jupyter mode\n",
        "spacy.displacy.render(doc_trf, style=\"ent\", jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSMk59EkxI0h"
      },
      "source": [
        "TRANSFORMERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7nRwhKoxBgl",
        "outputId": "6d5a5c7b-2950-42d0-f9cd-857f33bc3a09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.53)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhQIWTFo0gE2"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKkxC7aixBqd",
        "outputId": "7327622f-8376-471c-e7a9-9a5adb5f5c12"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/402f6d8c99fdd3bffd354782842e2b5a6be81f80ab630591051ebc78ca726f39.ebffac96fee44dbe30674c204dd3d3f358c1b8c33100281ecdd688514f41410a\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/28a060c1e2e1216bd9c8f5222ce38ce916c4829b8b05e027fe91510f3fd4da7e.50fc4a146342b3a6a99b185af3d5b70163b64d45790be64d9124dcccbcd3915e\n",
            "All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
            "\n",
            "All the weights of BertForQuestionAnswering were initialized from the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
            "loading file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/68e5260dea718cdc2daf27dc106fd8741636b03e3173b5492e57a7fa525ca33b.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/b9f8d92aa5a32cfe504c3524c173dc611dbe81d49392f40601286b94ee1e1169.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/402f6d8c99fdd3bffd354782842e2b5a6be81f80ab630591051ebc78ca726f39.ebffac96fee44dbe30674c204dd3d3f358c1b8c33100281ecdd688514f41410a\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
            "  \"architectures\": [\n",
            "    \"BertForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Step 3: Load pre-trained Bert model\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "tokenizer_for_bert = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_question_answer(question, passage, max_len=500):\n",
        "    \n",
        "    \"\"\"\n",
        "    question: What is the name of YouTube Channel\n",
        "    passage: Watch complete playlist of Natural Language Processing. Don't forget to like, share and subscribe my channel IG Tech Team\n",
        "    \"\"\"\n",
        "\n",
        "    #Tokenize input question and passage \n",
        "    #Add special tokens - [CLS] and [SEP]\n",
        "    input_ids = tokenizer_for_bert.encode (question, passage,  max_length= max_len, truncation=True)  \n",
        "    \"\"\"\n",
        "    [101, 2054, 2003, 1996, 2171, 1997, 7858, 3149, 102, 3422, 3143, 2377, 9863, 1997, 3019, 2653, 6364, 1012, \n",
        "    2123, 1005, 1056, 5293, 2000, 2066, 1010, 3745, 1998, 4942, 29234, 2026, 3149, 1045, 2290, 6627, 2136, 102]\n",
        "    \"\"\"\n",
        "\n",
        "    #Getting number of tokens in 1st sentence (question) and 2nd sentence (passage that contains answer)\n",
        "    sep_index = input_ids.index(102) \n",
        "    len_question = sep_index + 1   \n",
        "    len_passage = len(input_ids)- len_question  \n",
        "    \"\"\"\n",
        "    8\n",
        "    9\n",
        "    27\n",
        "    \"\"\"\n",
        "    #Need to separate question and passage\n",
        "    #Segment ids will be 0 for question and 1 for passage\n",
        "    segment_ids =  [0]*len_question + [1]*(len_passage)  \n",
        "    \"\"\"\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "    \"\"\"\n",
        "\n",
        "    #Converting token ids to tokens\n",
        "    tokens = tokenizer_for_bert.convert_ids_to_tokens(input_ids) \n",
        "    \"\"\"\n",
        "    tokens = ['[CLS]', 'what', 'is', 'the', 'name', 'of', 'youtube', 'channel', '[SEP]', 'watch', 'complete', \n",
        "    'play', '##list', 'of', 'natural', 'language', 'processing', '.', 'don', \"'\", 't', 'forget', 'to', 'like', \n",
        "    ',', 'share', 'and', 'sub', '##scribe', 'my', 'channel', 'i', '##g', 'tech', 'team', '[SEP]']\n",
        "    \"\"\"\n",
        "\n",
        "    #Getting start and end scores for answer\n",
        "    #Converting input arrays to torch tensors before passing to the model\n",
        "    start_token_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]) )[0]\n",
        "    end_token_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]) )[1]\n",
        "    \"\"\"\n",
        "    tensor([[-5.9787, -3.0541, -7.7166, -5.9291, -6.8790, -7.2380, -1.8289, -8.1006,\n",
        "         -5.9786, -3.9319, -5.6230, -4.1919, -7.2068, -6.7739, -2.3960, -5.9425,\n",
        "         -5.6828, -8.7007, -4.2650, -8.0987, -8.0837, -7.1799, -7.7863, -5.1605,\n",
        "         -8.2832, -5.1088, -8.1051, -5.3985, -6.7129, -1.4109, -3.2241,  1.5863,\n",
        "         -4.9714, -4.1138, -5.9107, -5.9786]], grad_fn=<SqueezeBackward1>)\n",
        "    tensor([[-2.1025, -2.9121, -5.9192, -6.7459, -6.4667, -5.6418, -1.4504, -3.1943,\n",
        "         -2.1024, -5.7470, -6.3381, -5.8520, -3.4871, -6.7667, -5.4711, -3.9885,\n",
        "         -1.2502, -4.0869, -6.4930, -6.3751, -6.1309, -6.9721, -7.5558, -6.4056,\n",
        "         -6.7456, -5.0527, -7.3854, -7.0440, -4.3720, -3.8936, -2.1085, -5.8211,\n",
        "         -2.0906, -2.2184,  1.4268, -2.1026]], grad_fn=<SqueezeBackward1>)\n",
        "    \"\"\"\n",
        "\n",
        "    #Converting scores tensors to numpy arrays\n",
        "    start_token_scores = start_token_scores.detach().numpy().flatten()\n",
        "    end_token_scores = end_token_scores.detach().numpy().flatten()\n",
        "    \"\"\"\n",
        "    [-5.978666  -3.0541189 -7.7166095 -5.929051  -6.878973  -7.238004\n",
        "    -1.8289301 -8.10058   -5.9786286 -3.9319289 -5.6229596 -4.191908\n",
        "    -7.20684   -6.773916  -2.3959794 -5.942456  -5.6827617 -8.700695\n",
        "    -4.265001  -8.09874   -8.083673  -7.179875  -7.7863474 -5.16046\n",
        "    -8.283156  -5.108819  -8.1051235 -5.3984528 -6.7128663 -1.4108785\n",
        "    -3.2240815  1.5863497 -4.9714    -4.113782  -5.9107194 -5.9786243]\n",
        "\n",
        "    [-2.1025064 -2.912148  -5.9192414 -6.745929  -6.466673  -5.641759\n",
        "    -1.4504088 -3.1943028 -2.1024144 -5.747039  -6.3380575 -5.852047\n",
        "    -3.487066  -6.7667046 -5.471078  -3.9884708 -1.2501552 -4.0868535\n",
        "    -6.4929943 -6.375147  -6.130891  -6.972091  -7.5557766 -6.405638\n",
        "    -6.7455807 -5.0527067 -7.3854156 -7.043977  -4.37199   -3.8935976\n",
        "    -2.1084964 -5.8210607 -2.0906193 -2.2184045  1.4268283 -2.1025767]\n",
        "    \"\"\"\n",
        "    #Getting start and end index of answer based on highest scores\n",
        "    answer_start_index = np.argmax(start_token_scores)\n",
        "    answer_end_index = np.argmax(end_token_scores)\n",
        "    \"\"\"\n",
        "    31\n",
        "    34\n",
        "    \"\"\"\n",
        "\n",
        "    #Getting scores for start and end token of the answer\n",
        "    start_token_score = np.round(start_token_scores[answer_start_index], 2)\n",
        "    end_token_score = np.round(end_token_scores[answer_end_index], 2)\n",
        "    \"\"\"\n",
        "    1.59\n",
        "    1.43\n",
        "    \"\"\"\n",
        "\n",
        "    #Combining subwords starting with ## and get full words in output. \n",
        "    #It is because tokenizer breaks words which are not in its vocab.\n",
        "    answer = tokens[answer_start_index] \n",
        "    for i in range(answer_start_index + 1, answer_end_index + 1):\n",
        "        if tokens[i][0:2] == '##':  \n",
        "            answer += tokens[i][2:] \n",
        "        else:\n",
        "            answer += ' ' + tokens[i]  \n",
        "# If the answer didn't find in the passage\n",
        "    if ( answer_start_index == 0) or (start_token_score < 0 ) or  (answer == '[SEP]') or ( answer_end_index <  answer_start_index):\n",
        "        answer = \"Sorry!, I could not find an answer in the passage.\"\n",
        "    \n",
        "    return (answer_start_index, answer_end_index, start_token_score, end_token_score,  answer)\n",
        "\n",
        "#Testing function\n",
        "bert_question_answer(\"What is the name of YouTube Channel\", \"Watch complete playlist of Natural Language Processing. Don't forget to like, share and subscribe my channel IG Tech Team \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX_5188eAbct",
        "outputId": "3bc2d3bf-bd94-49ef-c0c1-42d57237f683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(31, 34, 1.59, 1.43, 'ig tech team')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0ek37D68H6O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aMR0hKVxB1p"
      },
      "outputs": [],
      "source": [
        "def bert_question_answer(question, passage, max_len=500):\n",
        "    \n",
        "    \"\"\"\n",
        "    question: What is the name of YouTube Channel\n",
        "    passage: Watch complete playlist of Ishika Nisha. Don't forget to like, share and subscribe my channel IN Tech Team\n",
        "    \"\"\"\n",
        "    #Tokenize input question and passage \n",
        "    #Add special tokens - [CLS] and [SEP]\n",
        "    input_ids = tokenizer_for_bert.encode (question, passage,  max_length= max_len, truncation=True)  \n",
        "    \"\"\"\n",
        "    [101, 2054, 2003, 1996, 2171, 1997, 7858, 3149, 102, 3422, 3143, 2377, 9863, 1997, 3019, 2653, 6364, 1012, \n",
        "    2123, 1005, 1056, 5293, 2000, 2066, 1010, 3745, 1998, 4942, 29234, 2026, 3149, 1045, 2290, 6627, 2136, 102]\n",
        "    \"\"\"\n",
        "    #Getting number of tokens in 1st sentence (question) and 2nd sentence (passage that contains answer)\n",
        "    sep_index = input_ids.index(102) \n",
        "    len_question = sep_index + 1   \n",
        "    len_passage = len(input_ids)- len_question  \n",
        "    \"\"\"\n",
        "    8\n",
        "    93\n",
        "    27\n",
        "    \"\"\"\n",
        "    #Need to separate question and passage\n",
        "    #Segment ids will be 0 for question and 1 for passage\n",
        "    segment_ids =  [0]*len_question + [1]*(len_passage)  \n",
        "    \"\"\"\n",
        "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "    \"\"\"\n",
        "    #Converting token ids to tokens\n",
        "    tokens = tokenizer_for_bert.convert_ids_to_tokens(input_ids) \n",
        "    \"\"\"\n",
        "    tokens = ['[CLS]', 'what', 'is', 'the', 'name', 'of', 'youtube', 'channel', '[SEP]', 'watch', 'complete', \n",
        "    'play', '##list', 'of', 'ishika', 'nisha', '.', 'don', \"'\", 't', 'forget', 'to', 'like', \n",
        "    ',', 'share', 'and', 'sub', '##scribe', 'my', 'channel', 'i', '##n', 'tech', 'team', '[SEP]']\n",
        "    \"\"\"\n",
        "    #Getting start and end scores for answer\n",
        "    #Converting input arrays to torch tensors before passing to the model\n",
        "    start_token_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]) )[0]\n",
        "    end_token_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]) )[1]\n",
        "    \"\"\"\n",
        "    tensor([[-5.9787, -3.0541, -7.7166, -5.9291, -6.8790, -7.2380, -1.8289, -8.1006,\n",
        "         -5.9786, -3.9319, -5.6230, -4.1919, -7.2068, -6.7739, -2.3960, -5.9425,\n",
        "         -5.6828, -8.7007, -4.2650, -8.0987, -8.0837, -7.1799, -7.7863, -5.1605,\n",
        "         -8.2832, -5.1088, -8.1051, -5.3985, -6.7129, -1.4109, -3.2241,  1.5863,\n",
        "         -4.9714, -4.1138, -5.9107, -5.9786]], grad_fn=<SqueezeBackward1>)\n",
        "    tensor([[-2.1025, -2.9121, -5.9192, -6.7459, -6.4667, -5.6418, -1.4504, -3.1943,\n",
        "         -2.1024, -5.7470, -6.3381, -5.8520, -3.4871, -6.7667, -5.4711, -3.9885,\n",
        "         -1.2502, -4.0869, -6.4930, -6.3751, -6.1309, -6.9721, -7.5558, -6.4056,\n",
        "         -6.7456, -5.0527, -7.3854, -7.0440, -4.3720, -3.8936, -2.1085, -5.8211,\n",
        "         -2.0906, -2.2184,  1.4268, -2.1026]], grad_fn=<SqueezeBackward1>)\n",
        "    \"\"\"\n",
        "    #Converting scores tensors to numpy arrays\n",
        "    start_token_scores = start_token_scores.detach().numpy().flatten()\n",
        "    end_token_scores = end_token_scores.detach().numpy().flatten()\n",
        "    \"\"\"\n",
        "    [-5.978666  -3.0541189 -7.7166095 -5.929051  -6.878973  -7.238004\n",
        "    -1.8289301 -8.10058   -5.9786286 -3.9319289 -5.6229596 -4.191908\n",
        "    -7.20684   -6.773916  -2.3959794 -5.942456  -5.6827617 -8.700695\n",
        "    -4.265001  -8.09874   -8.083673  -7.179875  -7.7863474 -5.16046\n",
        "    -8.283156  -5.108819  -8.1051235 -5.3984528 -6.7128663 -1.4108785\n",
        "    -3.2240815  1.5863497 -4.9714    -4.113782  -5.9107194 -5.9786243]\n",
        "\n",
        "    [-2.1025064 -2.912148  -5.9192414 -6.745929  -6.466673  -5.641759\n",
        "    -1.4504088 -3.1943028 -2.1024144 -5.747039  -6.3380575 -5.852047\n",
        "    -3.487066  -6.7667046 -5.471078  -3.9884708 -1.2501552 -4.0868535\n",
        "    -6.4929943 -6.375147  -6.130891  -6.972091  -7.5557766 -6.405638\n",
        "    -6.7455807 -5.0527067 -7.3854156 -7.043977  -4.37199   -3.8935976\n",
        "    -2.1084964 -5.8210607 -2.0906193 -2.2184045  1.4268283 -2.1025767]\n",
        "    \"\"\"\n",
        "    #Getting start and end index of answer based on highest scores\n",
        "    answer_start_index = np.argmax(start_token_scores)\n",
        "    answer_end_index = np.argmax(end_token_scores)\n",
        "    \"\"\"\n",
        "    31\n",
        "    34\n",
        "    \"\"\"\n",
        "\n",
        "    #Getting scores for start and end token of the answer\n",
        "    start_token_score = np.round(start_token_scores[answer_start_index], 2)\n",
        "    end_token_score = np.round(end_token_scores[answer_end_index], 2)\n",
        "    \"\"\"\n",
        "    1.59\n",
        "    1.43\n",
        "    \"\"\"\n",
        "\n",
        "    #Combining subwords starting with ## and get full words in output. \n",
        "    #It is because tokenizer breaks words which are not in its vocab.\n",
        "    answer = tokens[answer_start_index] \n",
        "    for i in range(answer_start_index + 1, answer_end_index + 1):\n",
        "        if tokens[i][0:2] == '##':  \n",
        "            answer += tokens[i][2:] \n",
        "        else:\n",
        "            answer += ' ' + tokens[i]  \n",
        "    # If the answer didn't find in the passage\n",
        "    if ( answer_start_index == 0) or (start_token_score < 0 ) or  (answer == '[SEP]') or ( answer_end_index <  answer_start_index):\n",
        "        answer = \"Sorry!, I could not find an answer in the passage.\"\n",
        "    \n",
        "    return (answer_start_index, answer_end_index, start_token_score, end_token_score,  answer)\n",
        "\n",
        "    #Testing function\n",
        "    bert_question_answer(\"What is the name of YouTube Channel\", \"Watch complete playlist of Ishika Nisha. Don't forget to like, share and subscribe my channel IN Tech Team \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(31, 34, 1.59, 1.43, 'in tech team')\n"
      ],
      "metadata": {
        "id": "N29JgHE_BhDG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPUdFX79z5Bn",
        "outputId": "3f1d3575-45b5-4571-fdf5-8c5b497f56f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkNCm-oGxB-f",
        "outputId": "321935e4-6699-407b-fe11-1cb1f8c8bd2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of the passage: 246 words\n",
            "\n",
            "Question 1:\n",
            " Who is Rabindranath Tagore\n",
            "\n",
            "Answer from BERT:  a bengali polymath \n",
            "\n",
            "\n",
            "Question 7:\n",
            " When was Rabindranath Tagore born\n",
            "\n",
            "Answer from BERT:  7 may 1861 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Let me define another passage\n",
        "passage= \"\"\"Rabindranath Tagore FRAS (Bengali: রবীন্দ্রনাথ ঠাকুর, /rəˈbɪndrənɑːt tæˈɡɔːr/ (listen); 7 May 1861 – 7 August 1941) was a Bengali polymath who worked as a poet, writer, playwright, composer, philosopher, social reformer and painter. He reshaped Bengali literature and music as well as Indian art with Contextual Modernism in the late 19th and early 20th centuries. Author of the \"profoundly sensitive, fresh and beautiful\" poetry of Gitanjali, he became in 1913 the first non-European and the first lyricist to win the Nobel Prize in Literature. Tagore's poetic songs were viewed as spiritual and mercurial; however, his \"elegant prose and magical poetry\" remain largely unknown outside Bengal. He was a fellow of the Royal Asiatic Society. Referred to as \"the Bard of Bengal\", Tagore was known by sobriquets: Gurudev, Kobiguru, Biswakobi.[a]\n",
        "\n",
        "A Bengali Brahmin from Calcutta with ancestral gentry roots in Burdwan district[9] and Jessore, Tagore wrote poetry as an eight-year-old. At the age of sixteen, he released his first substantial poems under the pseudonym Bhānusiṃha (\"Sun Lion\"), which were seized upon by literary authorities as long-lost classics. By 1877 he graduated to his first short stories and dramas, published under his real name. As a humanist, universalist, internationalist, and ardent anti-nationalist, he denounced the British Raj and advocated independence from Britain. As an exponent of the Bengal Renaissance, he advanced a vast canon that comprised paintings, sketches and doodles, hundreds of texts, and some two thousand songs; his legacy also endures in his founding of Visva-Bharati University.\"\"\"\n",
        "\n",
        "print (f'Length of the passage: {len(passage.split())} words')\n",
        "\n",
        "question =\"Who is Rabindranath Tagore\"\n",
        "print ('\\nQuestion 1:\\n', question)\n",
        "_, _ , _ , _, ans  = bert_question_answer( question, passage)\n",
        "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
        "\n",
        "\n",
        "\n",
        "question =\"When was Rabindranath Tagore born\"\n",
        "print ('\\nQuestion 7:\\n', question)\n",
        "_, _ , _ , _, ans  = bert_question_answer( question, passage)\n",
        "print('\\nAnswer from BERT: ', ans ,  '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2S1NJlFxCBW",
        "outputId": "8872716d-f523-4042-d422-1ede33e47a66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of the passage: 51 words\n",
            "\n",
            "Question 1:\n",
            " What is my name\n",
            "\n",
            "Answer from BERT:  ishika \n",
            "\n",
            "\n",
            "Question 2:\n",
            " Who is the father of Sakshi\n",
            "\n",
            "Answer from BERT:  pradip \n",
            "\n",
            "\n",
            "Question 3:\n",
            " With whom Ishika spend most of the time\n",
            "\n",
            "Answer from BERT:  sakshi \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Let me define one passage\n",
        "passage = \"\"\"Hello, I am Ishika. My friend name is Sakshi. He is the son of Pradip. I spend most of the time with Sakshi. \n",
        "He always call me by my nick name. Sakshi call me programmer. Except Sakshi, my other friend call me by my original name. \n",
        "Amrita is also my friend. \"\"\"\n",
        "\n",
        "print (f'Length of the passage: {len(passage.split())} words')\n",
        "\n",
        "question1 =\"What is my name\" \n",
        "print ('\\nQuestion 1:\\n', question1)\n",
        "_, _ , _ , _, ans  = bert_question_answer( question1, passage)\n",
        "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
        "\n",
        "\n",
        "question2 =\"Who is the father of Sakshi\"\n",
        "print ('\\nQuestion 2:\\n', question2)\n",
        "_, _ , _ , _, ans  = bert_question_answer( question2, passage)\n",
        "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
        "\n",
        "question3 =\"With whom Ishika spend most of the time\" \n",
        "print ('\\nQuestion 3:\\n', question3)\n",
        "_, _ , _ , _, ans  = bert_question_answer( question3, passage)\n",
        "print('\\nAnswer from BERT: ', ans ,  '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxQR11RQ1Sie",
        "outputId": "79fff064-5026-4ed5-e2b6-2ce9b88c76ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sorry!, I could not find an answer in the passage.\n"
          ]
        }
      ],
      "source": [
        "#@title Question-Answering Application { vertical-output: true }\n",
        "#@markdown ---\n",
        "question= \"name of the sons of Rabindranath Tagore\" #@param {type:\"string\"}\n",
        "3\n",
        "passage = \"\"\"Rabindranath Tagore FRAS (Bengali: রবীন্দ্রনাথ ঠাকুর, /rəˈbɪndrənɑːt tæˈɡɔːr/ (listen); 7 May 1861 – 7 August 1941) was a Bengali polymath who worked as a poet, writer, playwright, composer, philosopher, social reformer and painter. He reshaped Bengali literature and music as well as Indian art with Contextual Modernism in the late 19th and early 20th centuries. Author of the \"profoundly sensitive, fresh and beautiful\" poetry of Gitanjali, he became in 1913 the first non-European and the first lyricist to win the Nobel Prize in Literature. Tagore's poetic songs were viewed as spiritual and mercurial; however, his \"elegant prose and magical poetry\" remain largely unknown outside Bengal. He was a fellow of the Royal Asiatic Society. Referred to as \"the Bard of Bengal\", Tagore was known by sobriquets: Gurudev, Kobiguru, Biswakobi.[a]\n",
        "\n",
        "A Bengali Brahmin from Calcutta with ancestral gentry roots in Burdwan district[9] and Jessore, Tagore wrote poetry as an eight-year-old. At the age of sixteen, he released his first substantial poems under the pseudonym Bhānusiṃha (\"Sun Lion\"), which were seized upon by literary authorities as long-lost classics. By 1877 he graduated to his first short stories and dramas, published under his real name. As a humanist, universalist, internationalist, and ardent anti-nationalist, he denounced the British Raj and advocated independence from Britain. As an exponent of the Bengal Renaissance, he advanced a vast canon that comprised paintings, sketches and doodles, hundreds of texts, and some two thousand songs; his legacy also endures in his founding of Visva-Bharati University.\"\"\"\n",
        "\n",
        "#@markdown ---\n",
        "_, _ , _ , _, ans  = bert_question_answer( question, passage)\n",
        "\n",
        "#@markdown Answer:\n",
        "print(ans)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OD8ZIDu_30H",
        "outputId": "0e69b1bc-0e25-4a14-f336-792d49678ece"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "albert einstein was a german - born theoretical physicist , widely acknowledged to be one of the greatest and most influential physicists of all time\n"
          ]
        }
      ],
      "source": [
        "#@title Question-Answering Application { vertical-output: true }\n",
        "#@markdown ---\n",
        "question= \"who is Albert Einstein\" #@param {type:\"string\"}\n",
        "3\n",
        "passage = \"\"\"Albert Einstein was a German-born theoretical physicist, widely acknowledged to be one of the greatest and most influential physicists of all time. Einstein is best known for developing the theory of relativity, but he also made important contributions to the development of the theory of quantum mechanics. Wikipedia\"\"\"\n",
        "#@markdown ---\n",
        "_, _ , _ , _, ans  = bert_question_answer( question, passage)\n",
        "\n",
        "#@markdown Answer:\n",
        "print(ans)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FINAL20.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}